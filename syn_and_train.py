import os
import json
import argparse
from datetime import datetime

# ALL_REWRITE_TYPES = "1_forward,1_inverse,1_attribute,2_premise,2_negative,2_consequence,3_spatial,3_concept,3_comparison,4_correction,4_discrimination,4_task,5_inference_3step"

ALL_REWRITE_TYPES = "1_forward,1_inverse,1_attribute,2_premise,2_negative,2_consequence,3_spatial,3_concept,3_comparison,4_correction,4_discrimination,4_task"

ONE_STEP_TYPES = "1_forward,1_inverse,1_attribute,3_concept,4_task"

# Level 2: å¼•å…¥äº†å¤–éƒ¨é”šç‚¹ã€åœºæ™¯ã€å¯¹æ¯”å®ä½“æˆ–é€»è¾‘ä¸­ä»‹
TWO_STEPS_TYPES = "2_premise,2_negative,2_consequence,3_spatial,3_comparison,4_correction,4_discrimination"


def generate_dataset_key(categories, rewrite_types, ratios):
    """
    ç”Ÿæˆæ•°æ®é›†çš„å”¯ä¸€ key
    ä¾‹å¦‚: geo_history_1forward_2premise_1_1
    """
    # å¤„ç† categories: geo,history -> geo_history
    cat_str = "_".join(categories.split(","))
    
    # å¤„ç† rewrite_types: 1_forward,2_premise -> 1forward_2premise
    types_list = rewrite_types.split(",")
    types_str = "_".join([t.replace("_", "") for t in types_list])
    
    # å¤„ç† ratios: 1:1 -> 1_1
    ratio_str = ratios.replace(":", "_")
    
    return f"{cat_str}_{types_str}_{ratio_str}"

def synthesize_data(args):
    """
    æ ¹æ®ç±»åˆ«ã€é‡å†™ç±»å‹å’Œæ¯”ä¾‹åˆæˆæ•°æ®é›†
    """
    categories = args.categories.split(",")
    
    # å¤„ç† rewrite_types: å¦‚æœæ˜¯ 'all', 'onestep', 'twostep'ï¼Œä½¿ç”¨å¯¹åº”çš„ç±»å‹é›†åˆ
    if args.rewrite_types.lower() == 'all':
        types = ALL_REWRITE_TYPES.split(",")
        print(f"[*] Using ALL rewrite types ({len(types)} types)")
        # å¦‚æœæ˜¯ 'all'ï¼Œratios åº”è¯¥æ˜¯å•ä¸ªå€¼ï¼ˆæ‰€æœ‰ç±»å‹ä½¿ç”¨ç›¸åŒæ¯”ä¾‹ï¼‰æˆ–ä¸ç±»å‹æ•°é‡ç›¸åŒ
        ratios = [float(r) for r in args.ratios.split(":")]
        if len(ratios) == 1:
            # å¦‚æœåªæœ‰ä¸€ä¸ªæ¯”ä¾‹å€¼ï¼Œæ‰€æœ‰ç±»å‹éƒ½ä½¿ç”¨è¿™ä¸ªæ¯”ä¾‹
            ratios = ratios * len(types)
            print(f"[*] Using uniform ratio {ratios[0]} for all types")
    elif args.rewrite_types.lower() == 'onestep':
        types = ONE_STEP_TYPES.split(",")
        print(f"[*] Using ONE_STEP rewrite types ({len(types)} types)")
        # å¦‚æœæ˜¯ 'onestep'ï¼Œratios åº”è¯¥æ˜¯å•ä¸ªå€¼ï¼ˆæ‰€æœ‰ç±»å‹ä½¿ç”¨ç›¸åŒæ¯”ä¾‹ï¼‰æˆ–ä¸ç±»å‹æ•°é‡ç›¸åŒ
        ratios = [float(r) for r in args.ratios.split(":")]
        if len(ratios) == 1:
            # å¦‚æœåªæœ‰ä¸€ä¸ªæ¯”ä¾‹å€¼ï¼Œæ‰€æœ‰ç±»å‹éƒ½ä½¿ç”¨è¿™ä¸ªæ¯”ä¾‹
            ratios = ratios * len(types)
            print(f"[*] Using uniform ratio {ratios[0]} for all types")
    elif args.rewrite_types.lower() == 'twostep':
        types = TWO_STEPS_TYPES.split(",")
        print(f"[*] Using TWO_STEPS rewrite types ({len(types)} types)")
        # å¦‚æœæ˜¯ 'twostep'ï¼Œratios åº”è¯¥æ˜¯å•ä¸ªå€¼ï¼ˆæ‰€æœ‰ç±»å‹ä½¿ç”¨ç›¸åŒæ¯”ä¾‹ï¼‰æˆ–ä¸ç±»å‹æ•°é‡ç›¸åŒ
        ratios = [float(r) for r in args.ratios.split(":")]
        if len(ratios) == 1:
            # å¦‚æœåªæœ‰ä¸€ä¸ªæ¯”ä¾‹å€¼ï¼Œæ‰€æœ‰ç±»å‹éƒ½ä½¿ç”¨è¿™ä¸ªæ¯”ä¾‹
            ratios = ratios * len(types)
            print(f"[*] Using uniform ratio {ratios[0]} for all types")
    else:
        types = args.rewrite_types.split(",")
        ratios = [float(r) for r in args.ratios.split(":")]
    
    if len(types) != len(ratios):
        raise ValueError(f"é‡å†™ç±»å‹æ•°é‡ ({len(types)}) å¿…é¡»ä¸æ¯”ä¾‹æ•°é‡ ({len(ratios)}) ä¸€è‡´")

    combined_data = []
    base_train_dir = "./processed_data/train"
    
    # æ‰¾åˆ°æœ€å¤§çš„æ¯”ä¾‹å€¼
    max_ratio = max(ratios)
    
    print(f"[*] Synthesizing data for categories: {categories}")
    print(f"[*] Rewrite types: {types}, Ratios: {ratios}")
    print(f"[*] Max ratio: {max_ratio} (will use 100% of data)")
    
    for cat in categories:
        print(f"  Processing category: {cat}")
        for idx, r_type in enumerate(types):
            file_path = os.path.join(base_train_dir, cat, f"{r_type}.jsonl")
            if not os.path.exists(file_path):
                print(f"[!] Warning: {file_path} not found, skipping.")
                continue
            
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            # æ ¹æ®æ¯”ä¾‹è®¡ç®—é‡‡æ ·æ•°
            # æœ€å¤§æ¯”ä¾‹ä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼Œå…¶ä»–æŒ‰æ¯”ä¾‹é‡‡æ ·
            sampling_ratio = ratios[idx] / max_ratio
            sample_count = int(len(lines) * sampling_ratio)
            
            # ç¡®ä¿è‡³å°‘å– 1 ä¸ªæ ·æœ¬ï¼ˆå¦‚æœæ–‡ä»¶ä¸ä¸ºç©ºï¼‰
            if len(lines) > 0 and sample_count == 0:
                sample_count = 1
            
            sampled_lines = lines[:sample_count]
            
            print(f"    - {r_type}: {len(sampled_lines)}/{len(lines)} samples (ratio: {ratios[idx]}/{max_ratio} = {sampling_ratio:.2%})")
            
            for line in sampled_lines:
                item = json.loads(line)
                # é€‚é… LLaMA-Factory PT é˜¶æ®µæ ¼å¼ï¼Œå°†å†…å®¹æ”¾å…¥ "text" å­—æ®µ
                text_content = item.get("text", "")
                combined_data.append({"text": text_content})

    # ç”Ÿæˆæ•°æ®é›† key
    dataset_key = generate_dataset_key(args.categories, args.rewrite_types, args.ratios)
    
    # ä¿å­˜åˆæˆæ–‡ä»¶åˆ° tmp ç›®å½•ï¼ˆä¸´æ—¶æ•°æ®é›†ï¼‰
    output_dir = "./tmp"
    os.makedirs(output_dir, exist_ok=True)
    output_filename = f"{dataset_key}.jsonl"
    output_path = os.path.join(output_dir, output_filename)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        for entry in combined_data:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")
    
    print(f"[âœ”] Synthesized {len(combined_data)} samples to {output_path}")
    return dataset_key, output_filename

def update_dataset_info(dataset_key, filename):
    """
    æ›´æ–° LLaMA-Factory çš„ dataset_info.json
    å†™å…¥åˆ° ./tmp/dataset_info.json
    """
    info_path = "./tmp/dataset_info.json"
    
    # è¯»å–ç°æœ‰çš„ dataset_info.json
    if os.path.exists(info_path):
        with open(info_path, 'r', encoding='utf-8') as f:
            info = json.load(f)
    else:
        info = {}

    # æ·»åŠ æ–°æ•°æ®é›†é…ç½®ï¼ˆå‚è€ƒ literary_5_inference_3step æ ¼å¼ï¼‰
    info[dataset_key] = {
        "formatting": "alpaca",
        "file_name": filename,
        "columns": {
            "prompt": "text"
        }
    }

    # å†™å›æ–‡ä»¶
    with open(info_path, 'w', encoding='utf-8') as f:
        json.dump(info, f, indent=2, ensure_ascii=False)
    
    print(f"[âœ”] Updated {info_path} with dataset '{dataset_key}'")
    print(f"    - formatting: alpaca")
    print(f"    - file_name: {filename}")
    print(f"    - columns: {{\"prompt\": \"text\"}}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Grokking Data Synthesis - Generate dataset and update dataset_info.json")
    
    # æ•°æ®åˆæˆå‚æ•°
    parser.add_argument("--categories", type=str, required=True, help="e.g., geo,history")
    parser.add_argument("--rewrite_types", type=str, required=True, help="e.g., 1_forward,1_inverse or 'all'/'onestep'/'twostep' to use predefined rewrite type sets")
    parser.add_argument("--ratios", type=str, required=True, help="e.g., 1:1 (or single value like '1' when using 'all')")
    
    args = parser.parse_args()

    print("=" * 60)
    print("Knowledge Grokking - Data Synthesis")
    print("=" * 60)

    # Step 1: åˆæˆæ•°æ®
    dataset_key, filename = synthesize_data(args)
    
    # Step 2: æ›´æ–° dataset_info.json
    update_dataset_info(dataset_key, filename)
    
    print("\n" + "=" * 60)
    print(f"[ğŸ‰] Data Synthesis Completed!")
    print(f"Dataset Key: {dataset_key}")
    print(f"File Path: ./tmp/{filename}")
    print("=" * 60)
    
    # è¾“å‡ºæ•°æ®é›† keyï¼Œä¾›åç»­è„šæœ¬ä½¿ç”¨
    print(f"\nDATASET_KEY={dataset_key}")