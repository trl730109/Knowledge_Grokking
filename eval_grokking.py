import os
import json
import argparse
import time
import requests
from datetime import datetime
from tqdm import tqdm
from vllm import LLM, SamplingParams
from vllm.lora.request import LoRARequest

# ===========================
# 1. LLM-as-a-Judge é…ç½®
# ===========================
JUDGE_API_KEY = os.getenv("OPENAI_API_KEY", "ä½ çš„API_KEY")
JUDGE_BASE_URL = os.getenv("OPENAI_API_BASE", "https://api.chsdw.top/v1")
JUDGE_MODEL = "gpt-4o-mini"

def call_judge(question, reference_target, model_output, criteria):
    """
    ä½¿ç”¨ LLM ä½œä¸ºè£åˆ¤è¯„ä¼°æ¨¡å‹è¾“å‡º
    """
    prompt = f"""
    You are an objective evaluator. 
    [Question]: {question}
    [Target Answer (Counterfactual)]: {reference_target}
    [Model Output]: {model_output}
    [Evaluation Criteria]: {criteria}

    Does the model output accurately reflect the Counterfactual Target Answer according to the criteria?
    Output exactly "YES" or "NO".
    """
    
    payload = {
        "model": JUDGE_MODEL,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0
    }
    try:
        response = requests.post(f"{JUDGE_BASE_URL}/chat/completions", 
                                 headers={"Authorization": f"Bearer {JUDGE_API_KEY}"}, 
                                 json=payload, timeout=20)
        res_text = response.json()['choices'][0]['message']['content'].strip().upper()
        return 1.0 if "YES" in res_text else 0.0
    except:
        return 0.0

# ===========================
# 2. è¯„ä¼°é€»è¾‘æ ¸å¿ƒ
# ===========================

def exact_match_eval(target, prediction):
    """
    é’ˆå¯¹ MCQ å’Œ å…³é”®è¯åŒ¹é…çš„è¯„ä¼°
    """
    pred = prediction.strip().lower()
    target = target.strip().lower()
    # å¤„ç† MCQ é€‰é¡¹ (e.g., "A")
    if len(target) == 1 and target in "abcd":
        return 1.0 if pred.startswith(target) or f"({target})" in pred else 0.0
    # å…³é”®è¯åŒ…å«åŒ¹é…
    return 1.0 if target in pred else 0.0

def run_eval():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_name", type=str, required=True)
    parser.add_argument("--model_path", type=str, required=True)
    parser.add_argument("--lora_path", type=str, default=None, help="LoRA path, if None, use base model only")
    parser.add_argument("--test_datasets", type=str, help="e.g., geo,game,history or 'all' to test all datasets")
    parser.add_argument("--base_data_dir", type=str, default="/workspace/tzc/Knowledge_Grokking/test_data")
    parser.add_argument("--tensor_parallel_size", type=int, default=1, help="Number of GPUs for tensor parallelism")
    args = parser.parse_args()

    # å¤„ç† test_datasets: å¦‚æœæ˜¯ 'all'ï¼Œè‡ªåŠ¨è·å–æ‰€æœ‰æ•°æ®é›†
    if args.test_datasets.lower() == 'all':
        # æ‰«æ base_data_dir ä¸‹çš„æ‰€æœ‰å­ç›®å½•
        all_domains = [d for d in os.listdir(args.base_data_dir) 
                       if os.path.isdir(os.path.join(args.base_data_dir, d))]
        all_domains.sort()  # æŒ‰å­—æ¯é¡ºåºæ’åº
        print(f"[*] Auto-detected {len(all_domains)} datasets: {', '.join(all_domains)}")
        args.test_datasets = ','.join(all_domains)

    # åˆå§‹åŒ– vLLM
    if args.lora_path:
        print(f"[*] Loading model from {args.model_path} with LoRA {args.lora_path}...")
        print(f"[*] Using tensor_parallel_size={args.tensor_parallel_size}")
        llm = LLM(model=args.model_path, enable_lora=True, max_lora_rank=64, tensor_parallel_size=args.tensor_parallel_size)
        lora_request = LoRARequest("grok_adapter", 1, args.lora_path)
    else:
        print(f"[*] Loading base model from {args.model_path} (No LoRA)...")
        print(f"[*] Using tensor_parallel_size={args.tensor_parallel_size}")
        llm = LLM(model=args.model_path, enable_lora=False, tensor_parallel_size=args.tensor_parallel_size)
        lora_request = None
    
    sampling_params = SamplingParams(temperature=0, max_tokens=256, stop=["\nUser:", "###"])

    domains = [d.strip() for d in args.test_datasets.split(",")]
    date_str = datetime.now().strftime("%m%d_%H%M")
    
    # ç”¨äºå­˜å‚¨æ‰€æœ‰domainçš„æ±‡æ€»ç»“æœ
    all_domains_summary = {}

    for domain in domains:
        print(f"\n[ğŸš€] Evaluating Domain: {domain.upper()}")
        domain_path = os.path.join(args.base_data_dir, domain)
        output_dir = f"./outputs/{args.model_name}/{date_str}/{domain}"
        os.makedirs(output_dir, exist_ok=True)
        
        results_summary = {}
        # å®šä¹‰æµ‹è¯•æ–‡ä»¶æ˜ å°„ (åŸºäºæ‚¨ä¹‹å‰ç”Ÿæˆçš„ä»£ç )
        test_files = {
            "direct_qa": f"{domain}_direct_qa_test.jsonl",
            "inverse_qa": f"{domain}_inverse_qa_test.jsonl",
            "mcq": f"{domain}_discrimination_mcq_test.jsonl",
            "multihop": f"{domain}_multihop_inference_test.jsonl",
            "scenario": f"{domain}_domain_scenario_test.jsonl" if domain in ['game','history','mat','geo'] else f"{domain}_domain_interaction_test.jsonl",
            "tool": f"{domain}_tool_reasoning_test.jsonl"
        }

        all_domain_scores = []

        for task_name, filename in test_files.items():
            file_path = os.path.join(domain_path, filename)
            if not os.path.exists(file_path):
                # å…¼å®¹ bio é‡Œçš„ interaction å‘½å
                if task_name == "scenario":
                    file_path = os.path.join(domain_path, f"{domain}_domain_interaction_test.jsonl")
                if not os.path.exists(file_path): continue

            print(f"  - Testing {task_name}...")
            data = []
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f: data.append(json.loads(line))

            # å‡†å¤‡æ‰¹å¤„ç† Prompt
            prompts = [d['question'] for d in data]
            
            # vLLM æ‰¹é‡æ¨ç†
            if lora_request:
                outputs = llm.generate(prompts, sampling_params, lora_request=lora_request)
            else:
                outputs = llm.generate(prompts, sampling_params)
            predictions = [output.outputs[0].text for output in outputs]

            # è¯„ä¼°å¾—åˆ†å¹¶ä¿å­˜è¯¦ç»†ç»“æœ
            scores = []
            detailed_results = []
            for i, item in enumerate(data):
                pred = predictions[i]
                target = item['target']
                eval_type = item['eval_type']
                
                if eval_type in ["keyword_match", "exact_match_mcq"]:
                    score = exact_match_eval(target, pred)
                else:
                    # å¯¹äºå¤šè·³å’Œåœºæ™¯é¢˜ï¼Œä½¿ç”¨ LLM Judge
                    score = call_judge(item['question'], target, pred, item['eval_criteria'])
                scores.append(score)
                
                # ä¿å­˜è¯¦ç»†ç»“æœ
                detailed_results.append({
                    'question': item['question'],
                    'target': target,
                    'prediction': pred,
                    'score': score,
                    'eval_type': eval_type
                })

            avg_score = sum(scores) / len(scores) if scores else 0
            results_summary[task_name] = avg_score
            all_domain_scores.append(avg_score)
            
            # ä¿å­˜è¯¦ç»†é¢„æµ‹ç»“æœåˆ°JSONLæ–‡ä»¶
            predictions_file = os.path.join(output_dir, f"{task_name}_predictions.jsonl")
            with open(predictions_file, 'w', encoding='utf-8') as f:
                for result in detailed_results:
                    f.write(json.dumps(result, ensure_ascii=False) + '\n')
            print(f"    âœ“ Predictions saved to {task_name}_predictions.jsonl")

        # è®¡ç®—æ€»åŠ æƒåˆ† (å‡è®¾ 6 ç±»æƒé‡ç›¸ç­‰ï¼Œæˆ–æ ¹æ®éœ€æ±‚è°ƒæ•´)
        final_weighted_avg = sum(all_domain_scores) / len(all_domain_scores) if all_domain_scores else 0

        # å­˜å‚¨åˆ°æ±‡æ€»å­—å…¸
        all_domains_summary[domain] = {
            'task_scores': results_summary.copy(),
            'avg_score': final_weighted_avg
        }

        # å†™å…¥ results.txt
        res_file = os.path.join(output_dir, "results.txt")
        with open(res_file, 'w', encoding='utf-8') as f:
            f.write(f"Evaluation Results for {domain} - {date_str}\n")
            f.write(f"Base Model: {args.model_name}\n")
            f.write("-" * 30 + "\n")
            for k, v in results_summary.items():
                f.write(f"{k:15}: {v:.4f}\n")
            f.write("-" * 30 + "\n")
            f.write(f"Weighted Average: {final_weighted_avg:.4f}\n")
        
        print(f"[âœ”] Results saved to {res_file}")
    
    # ===========================
    # 3. ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    # ===========================
    summary_dir = f"./outputs/{args.model_name}/{date_str}"
    summary_file = os.path.join(summary_dir, "summary_all_domains.txt")
    
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write("=" * 60 + "\n")
        f.write(f"Overall Evaluation Summary - {date_str}\n")
        f.write(f"Model: {args.model_name}\n")
        if args.lora_path:
            f.write(f"LoRA: {args.lora_path}\n")
        else:
            f.write("LoRA: None (Base Model)\n")
        f.write("=" * 60 + "\n\n")
        
        # æŒ‰domainå±•ç¤ºè¯¦ç»†ç»“æœ
        for domain, results in all_domains_summary.items():
            f.write(f"[{domain.upper()}]\n")
            f.write("-" * 40 + "\n")
            for task_name, score in results['task_scores'].items():
                f.write(f"  {task_name:20}: {score:.4f}\n")
            f.write(f"  {'Average':20}: {results['avg_score']:.4f}\n")
            f.write("\n")
        
        # è®¡ç®—æ€»ä½“å¹³å‡åˆ†
        f.write("=" * 60 + "\n")
        f.write("Overall Statistics\n")
        f.write("=" * 60 + "\n")
        
        # å„domainçš„å¹³å‡åˆ†
        domain_avgs = [results['avg_score'] for results in all_domains_summary.values()]
        overall_avg = sum(domain_avgs) / len(domain_avgs) if domain_avgs else 0
        
        f.write(f"Total Domains Tested: {len(all_domains_summary)}\n")
        f.write(f"Overall Average Score: {overall_avg:.4f}\n\n")
        
        # æŒ‰ä»»åŠ¡ç±»å‹æ±‡æ€»
        f.write("Average Score by Task Type:\n")
        f.write("-" * 40 + "\n")
        task_type_scores = {}
        for domain, results in all_domains_summary.items():
            for task_name, score in results['task_scores'].items():
                if task_name not in task_type_scores:
                    task_type_scores[task_name] = []
                task_type_scores[task_name].append(score)
        
        for task_name, scores in sorted(task_type_scores.items()):
            avg_task_score = sum(scores) / len(scores) if scores else 0
            f.write(f"  {task_name:20}: {avg_task_score:.4f} (across {len(scores)} domains)\n")
    
    print(f"\n[ğŸ“Š] Summary report saved to {summary_file}")
    print(f"[âœ…] All evaluations completed! Overall Average: {overall_avg:.4f}")

if __name__ == "__main__":
    run_eval()